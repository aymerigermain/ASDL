"""
=========================================================================================
IMAGE CAPTIONING AVEC PYTORCH - VERSION OPTIMIS√âE
=========================================================================================

Ce script impl√©mente un mod√®le d'image captioning optimis√© pour un entra√Ænement rapide.

OPTIMISATIONS PRINCIPALES :
1. Pr√©-extraction des features CNN avec cache (gain ~50x)
2. Batch size augment√© pour mieux utiliser le GPU (gain ~1.5x)
3. DataLoader avec num_workers et pin_memory (gain ~1.3x)
4. Mixed Precision Training (AMP) (gain ~2x)
5. Gradient accumulation pour stabilit√©
6. OneCycleLR scheduler pour convergence rapide

TEMPS ATTENDU : ~2-5 minutes par epoch (vs 1h+ sans optimisations)
=========================================================================================
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
import pandas as pd
from collections import Counter
from tqdm import tqdm
import matplotlib.pyplot as plt

# =========================================================================================
# 0. CONFIGURATION
# =========================================================================================

# Chemins des donn√©es
DATA_DIR = "/content/Flicker8k_Dataset"  # Modifier selon votre environnement
CAPTIONS_FILE = "/content/Flickr8k.token.txt"  # Modifier selon votre environnement
FEATURES_CACHE = "flickr8k_features_resnet50.pth"  # Cache des features CNN

# Hyperparam√®tres optimis√©s
BATCH_SIZE = 128  # ‚úÖ Augment√© pour mieux utiliser le GPU
NUM_WORKERS = 2   # ‚úÖ Chargement parall√®le des donn√©es
NUM_EPOCHS = 5
LEARNING_RATE = 5e-4
MAX_SEQ_LENGTH = 50
EMBED_DIM = 256
HIDDEN_DIM = 512
NUM_LAYERS = 2
DROPOUT = 0.3
ACCUMULATION_STEPS = 4  # ‚úÖ Gradient accumulation pour stabilit√©

# Configuration du device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"üöÄ Device utilis√© : {device}")
if torch.cuda.is_available():
    print(f"   GPU : {torch.cuda.get_device_name(0)}")
    print(f"   M√©moire disponible : {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

# =========================================================================================
# 1. CHARGEMENT DES DONN√âES
# =========================================================================================

def load_captions(captions_file):
    """
    Charge les l√©gendes depuis le fichier Flickr8k.token.txt
    Format : image.jpg#0  caption text

    Returns:
        pandas.DataFrame avec colonnes ['image', 'caption']
    """
    print("\nüìÇ Chargement des l√©gendes...")

    data = []
    with open(captions_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue

            # S√©paration image#num et caption
            parts = line.split('\t')
            if len(parts) != 2:
                continue

            image_info, caption = parts
            # Extraction du nom de l'image (sans #0, #1, etc.)
            image_name = image_info.split('#')[0]

            data.append({
                'image': image_name,
                'caption': caption.strip()
            })

    df = pd.DataFrame(data)
    print(f"‚úÖ {len(df)} l√©gendes charg√©es pour {df['image'].nunique()} images")
    print(f"   Moyenne : {len(df) / df['image'].nunique():.1f} l√©gendes par image")

    return df

# =========================================================================================
# 2. PR√â-TRAITEMENT DES TEXTES
# =========================================================================================

class TextPreprocessor:
    """
    Pr√©processeur pour convertir les l√©gendes en s√©quences d'entiers.
    G√®re le vocabulaire et les tokens sp√©ciaux.
    """

    def __init__(self, min_word_freq=2):
        """
        Args:
            min_word_freq: Fr√©quence minimale pour qu'un mot soit dans le vocabulaire
        """
        self.min_word_freq = min_word_freq
        self.word_to_idx = {}
        self.idx_to_word = {}
        self.vocab_size = 0

    def build_vocabulary(self, captions):
        """
        Construit le vocabulaire √† partir des l√©gendes.

        Args:
            captions: Liste de l√©gendes (strings)
        """
        print("\nüìö Construction du vocabulaire...")

        # Comptage des mots
        word_counts = Counter()
        for caption in captions:
            words = caption.lower().split()
            word_counts.update(words)

        # Filtrage par fr√©quence
        valid_words = [word for word, count in word_counts.items()
                      if count >= self.min_word_freq]

        # Cr√©ation des dictionnaires (tokens sp√©ciaux en premier)
        special_tokens = ['<PAD>', '<START>', '<END>', '<UNK>']
        self.word_to_idx = {token: idx for idx, token in enumerate(special_tokens)}

        for word in sorted(valid_words):
            if word not in self.word_to_idx:
                self.word_to_idx[word] = len(self.word_to_idx)

        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}
        self.vocab_size = len(self.word_to_idx)

        print(f"‚úÖ Vocabulaire construit : {self.vocab_size} mots")
        print(f"   Mots filtr√©s (freq < {self.min_word_freq}) : {len(word_counts) - len(valid_words)}")

    def caption_to_sequence(self, caption):
        """
        Convertit une l√©gende en s√©quence d'indices.

        Args:
            caption: L√©gende (string)

        Returns:
            Liste d'indices (avec <START> et <END>)
        """
        words = caption.lower().split()
        sequence = [self.word_to_idx['<START>']]

        for word in words:
            idx = self.word_to_idx.get(word, self.word_to_idx['<UNK>'])
            sequence.append(idx)

        sequence.append(self.word_to_idx['<END>'])
        return sequence

    def sequence_to_caption(self, sequence):
        """
        Convertit une s√©quence d'indices en l√©gende.

        Args:
            sequence: Liste ou tensor d'indices

        Returns:
            L√©gende (string)
        """
        if torch.is_tensor(sequence):
            sequence = sequence.cpu().numpy()

        words = []
        for idx in sequence:
            word = self.idx_to_word.get(int(idx), '<UNK>')
            if word in ['<START>', '<PAD>']:
                continue
            if word == '<END>':
                break
            words.append(word)

        return ' '.join(words)

# =========================================================================================
# 3. EXTRACTION DES FEATURES CNN (PR√â-EXTRACTION AVEC CACHE) ‚ö°
# =========================================================================================

class FeatureExtractorCNN:
    """
    Extracteur de features bas√© sur ResNet50 pr√©-entra√Æn√©.
    Les features sont extraites UNE SEULE FOIS et mises en cache.
    """

    def __init__(self, model_name='resnet50', device='cuda'):
        """
        Args:
            model_name: Nom du mod√®le ('resnet50' ou 'vgg16')
            device: Device PyTorch
        """
        self.device = device
        self.model_name = model_name

        print(f"\nüîß Initialisation de l'extracteur de features ({model_name})...")

        # Chargement du mod√®le pr√©-entra√Æn√©
        if model_name == 'resnet50':
            model = models.resnet50(pretrained=True)
            # Suppression de la couche de classification
            self.model = nn.Sequential(*list(model.children())[:-1])
            self.feature_dim = 2048
        elif model_name == 'vgg16':
            model = models.vgg16(pretrained=True)
            self.model = nn.Sequential(*list(model.children())[:-1])
            self.feature_dim = 25088
        else:
            raise ValueError(f"Mod√®le non support√© : {model_name}")

        self.model.eval()
        self.model.to(device)

        # Transformation des images (normalization ImageNet)
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                               std=[0.229, 0.224, 0.225])
        ])

        print(f"‚úÖ Extracteur pr√™t (dimension des features : {self.feature_dim})")

    def extract_single(self, image_path):
        """
        Extrait les features d'une seule image.

        Args:
            image_path: Chemin vers l'image

        Returns:
            Tensor de features (feature_dim,)
        """
        try:
            # Chargement et transformation
            img = Image.open(image_path).convert('RGB')
            img_tensor = self.transform(img).unsqueeze(0).to(self.device)

            # Extraction
            with torch.no_grad():
                features = self.model(img_tensor).view(-1)

            return features.cpu()

        except Exception as e:
            print(f"‚ùå Erreur sur {image_path}: {e}")
            return torch.zeros(self.feature_dim)

    def extract_batch(self, image_paths, batch_size=64):
        """
        Extrait les features par batch (plus rapide).

        Args:
            image_paths: Liste de chemins d'images
            batch_size: Taille du batch

        Returns:
            Dict {image_name: features_tensor}
        """
        features_cache = {}

        print(f"üîç Extraction des features en batch (batch_size={batch_size})...")

        for i in tqdm(range(0, len(image_paths), batch_size)):
            batch_paths = image_paths[i:i+batch_size]
            batch_tensors = []
            valid_names = []

            # Chargement du batch
            for path in batch_paths:
                try:
                    img = Image.open(path).convert('RGB')
                    img_tensor = self.transform(img)
                    batch_tensors.append(img_tensor)
                    valid_names.append(os.path.basename(path))
                except Exception as e:
                    print(f"‚ùå Erreur sur {path}: {e}")
                    continue

            if len(batch_tensors) == 0:
                continue

            # Extraction batch
            batch_tensor = torch.stack(batch_tensors).to(self.device)

            with torch.no_grad():
                features = self.model(batch_tensor).view(batch_tensor.size(0), -1)

            # Stockage dans le cache
            for name, feat in zip(valid_names, features):
                features_cache[name] = feat.cpu()

        print(f"‚úÖ {len(features_cache)} features extraites")
        return features_cache


def load_or_extract_features(df, data_dir, cache_path, device):
    """
    Charge les features depuis le cache ou les extrait si n√©cessaire.

    Args:
        df: DataFrame avec la colonne 'image'
        data_dir: R√©pertoire des images
        cache_path: Chemin du fichier cache
        device: Device PyTorch

    Returns:
        Dict {image_name: features_tensor}
    """
    # Tentative de chargement depuis le cache
    if os.path.exists(cache_path):
        print(f"\nüíæ Chargement des features depuis le cache : {cache_path}")
        features_cache = torch.load(cache_path)
        print(f"‚úÖ {len(features_cache)} features charg√©es depuis le cache")
        return features_cache

    # Extraction des features
    print(f"\n‚ö†Ô∏è  Cache introuvable. Extraction des features (cela peut prendre quelques minutes)...")

    extractor = FeatureExtractorCNN(model_name='resnet50', device=device)

    # Liste des images uniques
    unique_images = df['image'].unique()
    image_paths = [os.path.join(data_dir, img) for img in unique_images]

    # Extraction par batch (beaucoup plus rapide)
    features_cache = extractor.extract_batch(image_paths, batch_size=64)

    # Sauvegarde dans le cache
    print(f"\nüíæ Sauvegarde des features dans le cache : {cache_path}")
    torch.save(features_cache, cache_path)
    print(f"‚úÖ Cache sauvegard√©. Prochains lancements seront instantan√©s !")

    return features_cache

# =========================================================================================
# 4. DIVISION DU DATASET
# =========================================================================================

def split_dataset(df, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, seed=42):
    """
    Divise le dataset en train/val/test au niveau des images.

    Args:
        df: DataFrame avec colonnes ['image', 'caption']
        train_ratio: Proportion du train set
        val_ratio: Proportion du validation set
        test_ratio: Proportion du test set
        seed: Seed pour reproductibilit√©

    Returns:
        train_df, val_df, test_df
    """
    print("\n‚úÇÔ∏è  Division du dataset...")

    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, "Les ratios doivent sommer √† 1.0"

    # Images uniques
    unique_images = df['image'].unique()
    total_images = len(unique_images)

    # Shuffle reproductible
    import numpy as np
    np.random.seed(seed)
    shuffled_images = np.random.permutation(unique_images)

    # Calcul des indices
    train_end = int(total_images * train_ratio)
    val_end = train_end + int(total_images * val_ratio)

    # S√©paration
    train_images = set(shuffled_images[:train_end])
    val_images = set(shuffled_images[train_end:val_end])
    test_images = set(shuffled_images[val_end:])

    # Cr√©ation des DataFrames
    train_df = df[df['image'].isin(train_images)].copy()
    val_df = df[df['image'].isin(val_images)].copy()
    test_df = df[df['image'].isin(test_images)].copy()

    print(f"‚úÖ Division effectu√©e :")
    print(f"   Train : {len(train_df)} l√©gendes ({len(train_images)} images)")
    print(f"   Val   : {len(val_df)} l√©gendes ({len(val_images)} images)")
    print(f"   Test  : {len(test_df)} l√©gendes ({len(test_images)} images)")

    return train_df, val_df, test_df

# =========================================================================================
# 5. DATASET PYTORCH OPTIMIS√â
# =========================================================================================

class ImageCaptionDataset(Dataset):
    """
    Dataset PyTorch optimis√© avec features pr√©-extraites.
    """

    def __init__(self, dataframe, preprocessor, features_cache, max_length):
        """
        Args:
            dataframe: DataFrame avec colonnes ['image', 'caption']
            preprocessor: Instance de TextPreprocessor
            features_cache: Dict {image_name: features_tensor}
            max_length: Longueur maximale des s√©quences
        """
        self.df = dataframe.reset_index(drop=True)
        self.preprocessor = preprocessor
        self.features_cache = features_cache
        self.max_length = max_length
        self.pad_idx = preprocessor.word_to_idx['<PAD>']

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        """
        Retourne un √©chantillon (features, input_seq, target_seq).

        Returns:
            dict avec cl√©s:
                - 'image_features': Tensor (feature_dim,)
                - 'input_seqs': Tensor (max_length,) - s√©quence d'entr√©e
                - 'target_seqs': Tensor (max_length,) - s√©quence cible
        """
        row = self.df.iloc[idx]

        # ‚úÖ R√©cup√©ration des features depuis le cache (instantan√©)
        image_features = self.features_cache[row['image']].float()

        # Conversion caption -> s√©quence
        seq = self.preprocessor.caption_to_sequence(row['caption'])

        # Padding
        if len(seq) > self.max_length:
            seq = seq[:self.max_length]
        else:
            seq = seq + [self.pad_idx] * (self.max_length - len(seq))

        # Input : <START> word1 word2 ... <END> <PAD>
        # Target:   word1 word2 ... <END> <PAD> <PAD>
        input_seq = seq[:-1]   # Tout sauf le dernier
        target_seq = seq[1:]   # Tout sauf le premier

        return {
            'image_features': image_features,
            'input_seqs': torch.tensor(input_seq, dtype=torch.long),
            'target_seqs': torch.tensor(target_seq, dtype=torch.long)
        }


def collate_fn(batch):
    """
    Fonction de collation pour le DataLoader.
    Empile les √©chantillons en batch.
    """
    image_features = torch.stack([item['image_features'] for item in batch])
    input_seqs = torch.stack([item['input_seqs'] for item in batch])
    target_seqs = torch.stack([item['target_seqs'] for item in batch])

    return {
        'image_features': image_features,
        'input_seqs': input_seqs,
        'target_seqs': target_seqs
    }

# =========================================================================================
# 6. ARCHITECTURE DU MOD√àLE
# =========================================================================================

class ImageCaptionModel(nn.Module):
    """
    Mod√®le d'image captioning avec LSTM.

    Architecture :
        - Projection des features CNN
        - Embedding des mots
        - LSTM pour la g√©n√©ration s√©quentielle
        - Couche de sortie (vocabulaire)
    """

    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers,
                 feature_dim, dropout=0.3):
        """
        Args:
            vocab_size: Taille du vocabulaire
            embed_dim: Dimension des embeddings
            hidden_dim: Dimension cach√©e du LSTM
            num_layers: Nombre de couches LSTM
            feature_dim: Dimension des features d'image
            dropout: Taux de dropout
        """
        super().__init__()

        self.vocab_size = vocab_size
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        # Projection des features d'image dans l'espace du LSTM
        self.feature_projection = nn.Sequential(
            nn.Linear(feature_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        # Embedding des mots
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)

        # LSTM
        self.lstm = nn.LSTM(
            input_size=embed_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )

        # Couche de sortie
        self.fc_out = nn.Linear(hidden_dim, vocab_size)

        self.dropout = nn.Dropout(dropout)

    def forward(self, image_features, input_sequences):
        """
        Forward pass.

        Args:
            image_features: Tensor (batch_size, feature_dim)
            input_sequences: Tensor (batch_size, seq_len)

        Returns:
            outputs: Tensor (batch_size, seq_len, vocab_size)
        """
        batch_size = image_features.size(0)

        # Projection des features d'image
        img_proj = self.feature_projection(image_features)  # (batch, hidden_dim)

        # Initialisation du hidden state avec les features d'image
        # h0 : (num_layers, batch, hidden_dim)
        h0 = img_proj.unsqueeze(0).repeat(self.num_layers, 1, 1)
        c0 = torch.zeros_like(h0)

        # Embedding des s√©quences d'entr√©e
        embeddings = self.embedding(input_sequences)  # (batch, seq_len, embed_dim)
        embeddings = self.dropout(embeddings)

        # Passage dans le LSTM
        lstm_out, _ = self.lstm(embeddings, (h0, c0))  # (batch, seq_len, hidden_dim)

        # Projection vers le vocabulaire
        outputs = self.fc_out(lstm_out)  # (batch, seq_len, vocab_size)

        return outputs

    def generate_caption(self, image_features, preprocessor, max_length=50, device='cuda'):
        """
        G√©n√®re une l√©gende pour une image (inf√©rence).

        Args:
            image_features: Tensor (1, feature_dim) ou (feature_dim,)
            preprocessor: Instance de TextPreprocessor
            max_length: Longueur maximale de g√©n√©ration
            device: Device PyTorch

        Returns:
            caption: String de la l√©gende g√©n√©r√©e
        """
        self.eval()

        with torch.no_grad():
            # Reshape si n√©cessaire
            if image_features.dim() == 1:
                image_features = image_features.unsqueeze(0)

            image_features = image_features.to(device)

            # Token de d√©part
            start_token = preprocessor.word_to_idx['<START>']
            end_token = preprocessor.word_to_idx['<END>']

            # S√©quence g√©n√©r√©e
            generated = [start_token]

            # G√©n√©ration token par token
            for _ in range(max_length):
                # S√©quence actuelle
                input_seq = torch.tensor([generated], dtype=torch.long, device=device)

                # Forward
                outputs = self.forward(image_features, input_seq)

                # Pr√©diction du prochain token (dernier token de la s√©quence)
                next_token_logits = outputs[0, -1, :]  # (vocab_size,)
                next_token = next_token_logits.argmax().item()

                # Ajout du token
                generated.append(next_token)

                # Arr√™t si <END>
                if next_token == end_token:
                    break

            # Conversion en texte
            caption = preprocessor.sequence_to_caption(generated)

            return caption

# =========================================================================================
# 7. ENTRA√éNEMENT OPTIMIS√â AVEC MIXED PRECISION ‚ö°
# =========================================================================================

class Trainer:
    """
    Classe d'entra√Ænement optimis√©e avec Mixed Precision Training.
    """

    def __init__(self, model, criterion, optimizer, scheduler, device,
                 accumulation_steps=4):
        """
        Args:
            model: Mod√®le PyTorch
            criterion: Fonction de perte
            optimizer: Optimiseur
            scheduler: Learning rate scheduler
            device: Device PyTorch
            accumulation_steps: Nombre de steps avant mise √† jour (gradient accumulation)
        """
        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.device = device
        self.accumulation_steps = accumulation_steps

        # ‚úÖ Scaler pour Mixed Precision
        self.scaler = GradScaler()

        # Historique
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'learning_rates': []
        }

    def train_epoch(self, dataloader):
        """
        Entra√Æne le mod√®le pour une √©poque.

        Args:
            dataloader: DataLoader d'entra√Ænement

        Returns:
            loss moyenne de l'√©poque
        """
        self.model.train()
        total_loss = 0
        num_batches = len(dataloader)

        # Remise √† z√©ro des gradients
        self.optimizer.zero_grad()

        progress_bar = tqdm(dataloader, desc="Training")

        for batch_idx, batch in enumerate(progress_bar):
            # Transfert sur GPU
            img_feats = batch['image_features'].to(self.device)
            in_seq = batch['input_seqs'].to(self.device)
            tgt_seq = batch['target_seqs'].to(self.device)

            # ‚úÖ Forward avec Mixed Precision (autocast)
            with autocast():
                outputs = self.model(img_feats, in_seq)

                # Calcul de la perte
                # outputs : (batch, seq_len, vocab_size)
                # tgt_seq : (batch, seq_len)
                loss = self.criterion(
                    outputs.reshape(-1, outputs.shape[2]),
                    tgt_seq.reshape(-1)
                )

                # Normalisation par accumulation steps
                loss = loss / self.accumulation_steps

            # ‚úÖ Backward avec scaling
            self.scaler.scale(loss).backward()

            # Mise √† jour tous les accumulation_steps
            if (batch_idx + 1) % self.accumulation_steps == 0:
                # ‚úÖ Unscale avant gradient clipping
                self.scaler.unscale_(self.optimizer)

                # Gradient clipping pour stabilit√©
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)

                # ‚úÖ Step avec scaler
                self.scaler.step(self.optimizer)
                self.scaler.update()

                # Remise √† z√©ro
                self.optimizer.zero_grad()

            # Accumulation de la perte
            total_loss += loss.item() * self.accumulation_steps

            # Affichage dans la progress bar
            progress_bar.set_postfix({'loss': f'{loss.item() * self.accumulation_steps:.4f}'})

        # Moyenne sur l'√©poque
        avg_loss = total_loss / num_batches

        return avg_loss

    def validate_epoch(self, dataloader):
        """
        Valide le mod√®le pour une √©poque.

        Args:
            dataloader: DataLoader de validation

        Returns:
            loss moyenne de l'√©poque
        """
        self.model.eval()
        total_loss = 0
        num_batches = len(dataloader)

        progress_bar = tqdm(dataloader, desc="Validation")

        with torch.no_grad():
            for batch in progress_bar:
                # Transfert sur GPU
                img_feats = batch['image_features'].to(self.device)
                in_seq = batch['input_seqs'].to(self.device)
                tgt_seq = batch['target_seqs'].to(self.device)

                # ‚úÖ Forward avec Mixed Precision
                with autocast():
                    outputs = self.model(img_feats, in_seq)

                    # Calcul de la perte
                    loss = self.criterion(
                        outputs.reshape(-1, outputs.shape[2]),
                        tgt_seq.reshape(-1)
                    )

                total_loss += loss.item()

                # Affichage dans la progress bar
                progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})

        # Moyenne sur l'√©poque
        avg_loss = total_loss / num_batches

        return avg_loss

    def train(self, train_loader, val_loader, num_epochs, save_path='best_model.pth'):
        """
        Boucle d'entra√Ænement compl√®te.

        Args:
            train_loader: DataLoader d'entra√Ænement
            val_loader: DataLoader de validation
            num_epochs: Nombre d'√©poques
            save_path: Chemin de sauvegarde du meilleur mod√®le
        """
        print(f"\nüöÄ D√©but de l'entra√Ænement ({num_epochs} √©poques)...\n")

        best_val_loss = float('inf')

        for epoch in range(num_epochs):
            print(f"\n{'='*70}")
            print(f"EPOCH {epoch + 1}/{num_epochs}")
            print(f"{'='*70}")

            # Entra√Ænement
            train_loss = self.train_epoch(train_loader)

            # Validation
            val_loss = self.validate_epoch(val_loader)

            # Mise √† jour du learning rate
            if self.scheduler is not None:
                self.scheduler.step()

            # Learning rate actuel
            current_lr = self.optimizer.param_groups[0]['lr']

            # Sauvegarde de l'historique
            self.history['train_loss'].append(train_loss)
            self.history['val_loss'].append(val_loss)
            self.history['learning_rates'].append(current_lr)

            # Affichage
            print(f"\nüìä R√©sultats Epoch {epoch + 1}:")
            print(f"   Train Loss : {train_loss:.4f}")
            print(f"   Val Loss   : {val_loss:.4f}")
            print(f"   LR         : {current_lr:.6f}")

            # Sauvegarde du meilleur mod√®le
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save({
                    'epoch': epoch + 1,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'val_loss': val_loss,
                    'history': self.history
                }, save_path)
                print(f"   ‚úÖ Meilleur mod√®le sauvegard√© ! (val_loss: {val_loss:.4f})")

        print(f"\n{'='*70}")
        print(f"‚úÖ ENTRA√éNEMENT TERMIN√â !")
        print(f"   Meilleure validation loss : {best_val_loss:.4f}")
        print(f"   Mod√®le sauvegard√© : {save_path}")
        print(f"{'='*70}\n")

        return self.history

# =========================================================================================
# 8. FONCTION PRINCIPALE
# =========================================================================================

def main():
    """
    Fonction principale pour lancer l'entra√Ænement complet.
    """

    print("\n" + "="*70)
    print("IMAGE CAPTIONING - VERSION OPTIMIS√âE")
    print("="*70)

    # ---------------------------------------------------------------------------------
    # √âTAPE 1 : Chargement des donn√©es
    # ---------------------------------------------------------------------------------
    df = load_captions(CAPTIONS_FILE)

    # ---------------------------------------------------------------------------------
    # √âTAPE 2 : Pr√©traitement des textes
    # ---------------------------------------------------------------------------------
    preprocessor = TextPreprocessor(min_word_freq=2)
    preprocessor.build_vocabulary(df['caption'].tolist())

    # ---------------------------------------------------------------------------------
    # √âTAPE 3 : Extraction/Chargement des features CNN ‚ö°
    # ---------------------------------------------------------------------------------
    features_cache = load_or_extract_features(
        df=df,
        data_dir=DATA_DIR,
        cache_path=FEATURES_CACHE,
        device=device
    )

    # ---------------------------------------------------------------------------------
    # √âTAPE 4 : Division du dataset
    # ---------------------------------------------------------------------------------
    train_df, val_df, test_df = split_dataset(
        df,
        train_ratio=0.8,
        val_ratio=0.1,
        test_ratio=0.1
    )

    # ---------------------------------------------------------------------------------
    # √âTAPE 5 : Cr√©ation des Datasets et DataLoaders optimis√©s
    # ---------------------------------------------------------------------------------
    print("\nüîß Cr√©ation des datasets et dataloaders...")

    train_dataset = ImageCaptionDataset(train_df, preprocessor, features_cache, MAX_SEQ_LENGTH)
    val_dataset = ImageCaptionDataset(val_df, preprocessor, features_cache, MAX_SEQ_LENGTH)
    test_dataset = ImageCaptionDataset(test_df, preprocessor, features_cache, MAX_SEQ_LENGTH)

    # ‚úÖ DataLoaders optimis√©s
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=NUM_WORKERS,
        pin_memory=True,  # ‚úÖ Transfert GPU plus rapide
        collate_fn=collate_fn
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=NUM_WORKERS,
        pin_memory=True,
        collate_fn=collate_fn
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=NUM_WORKERS,
        pin_memory=True,
        collate_fn=collate_fn
    )

    print(f"‚úÖ DataLoaders cr√©√©s :")
    print(f"   Train : {len(train_loader)} batches")
    print(f"   Val   : {len(val_loader)} batches")
    print(f"   Test  : {len(test_loader)} batches")

    # ---------------------------------------------------------------------------------
    # √âTAPE 6 : Cr√©ation du mod√®le
    # ---------------------------------------------------------------------------------
    print("\nüîß Cr√©ation du mod√®le...")

    # Dimension des features (ResNet50)
    feature_dim = list(features_cache.values())[0].shape[0]

    model = ImageCaptionModel(
        vocab_size=preprocessor.vocab_size,
        embed_dim=EMBED_DIM,
        hidden_dim=HIDDEN_DIM,
        num_layers=NUM_LAYERS,
        feature_dim=feature_dim,
        dropout=DROPOUT
    ).to(device)

    # Comptage des param√®tres
    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"‚úÖ Mod√®le cr√©√© : {num_params:,} param√®tres entra√Ænables")

    # ---------------------------------------------------------------------------------
    # √âTAPE 7 : Configuration de l'entra√Ænement
    # ---------------------------------------------------------------------------------
    print("\nüîß Configuration de l'entra√Ænement...")

    # Fonction de perte (ignore PAD tokens)
    pad_idx = preprocessor.word_to_idx['<PAD>']
    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)

    # Optimiseur
    optimizer = optim.AdamW(
        model.parameters(),
        lr=LEARNING_RATE,
        weight_decay=1e-4
    )

    # ‚úÖ OneCycleLR pour convergence rapide
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=LEARNING_RATE,
        epochs=NUM_EPOCHS,
        steps_per_epoch=len(train_loader),
        pct_start=0.1  # 10% du temps en warmup
    )

    print(f"‚úÖ Configuration pr√™te :")
    print(f"   Perte : CrossEntropyLoss (ignore PAD)")
    print(f"   Optimiseur : AdamW (lr={LEARNING_RATE}, weight_decay=1e-4)")
    print(f"   Scheduler : OneCycleLR (max_lr={LEARNING_RATE})")
    print(f"   Mixed Precision : Activ√© ‚ö°")
    print(f"   Gradient Accumulation : {ACCUMULATION_STEPS} steps")

    # ---------------------------------------------------------------------------------
    # √âTAPE 8 : Entra√Ænement
    # ---------------------------------------------------------------------------------
    trainer = Trainer(
        model=model,
        criterion=criterion,
        optimizer=optimizer,
        scheduler=scheduler,
        device=device,
        accumulation_steps=ACCUMULATION_STEPS
    )

    history = trainer.train(
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=NUM_EPOCHS,
        save_path='best_image_caption_model.pth'
    )

    # ---------------------------------------------------------------------------------
    # √âTAPE 9 : Visualisation des r√©sultats
    # ---------------------------------------------------------------------------------
    print("\nüìä Visualisation des r√©sultats...")

    plt.figure(figsize=(15, 5))

    # Loss
    plt.subplot(1, 3, 1)
    plt.plot(history['train_loss'], label='Train Loss', marker='o')
    plt.plot(history['val_loss'], label='Val Loss', marker='o')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Evolution de la Loss')
    plt.legend()
    plt.grid(True)

    # Learning Rate
    plt.subplot(1, 3, 2)
    plt.plot(history['learning_rates'], marker='o', color='green')
    plt.xlabel('Epoch')
    plt.ylabel('Learning Rate')
    plt.title('Evolution du Learning Rate')
    plt.grid(True)

    # Perplexity (exp(loss))
    plt.subplot(1, 3, 3)
    train_perplexity = [torch.exp(torch.tensor(loss)).item() for loss in history['train_loss']]
    val_perplexity = [torch.exp(torch.tensor(loss)).item() for loss in history['val_loss']]
    plt.plot(train_perplexity, label='Train Perplexity', marker='o')
    plt.plot(val_perplexity, label='Val Perplexity', marker='o')
    plt.xlabel('Epoch')
    plt.ylabel('Perplexity')
    plt.title('Evolution de la Perplexity')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig('training_results.png', dpi=150, bbox_inches='tight')
    print("‚úÖ Graphiques sauvegard√©s : training_results.png")

    # ---------------------------------------------------------------------------------
    # √âTAPE 10 : Test de g√©n√©ration
    # ---------------------------------------------------------------------------------
    print("\nüß™ Test de g√©n√©ration de l√©gendes...")

    # Chargement du meilleur mod√®le
    checkpoint = torch.load('best_image_caption_model.pth')
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    # Test sur quelques images
    test_samples = test_df.sample(5)

    for idx, row in test_samples.iterrows():
        image_name = row['image']
        true_caption = row['caption']

        # Features de l'image
        img_features = features_cache[image_name]

        # G√©n√©ration
        generated_caption = model.generate_caption(
            img_features,
            preprocessor,
            max_length=MAX_SEQ_LENGTH,
            device=device
        )

        print(f"\n{'='*70}")
        print(f"Image : {image_name}")
        print(f"Vraie l√©gende  : {true_caption}")
        print(f"L√©gende g√©n√©r√©e : {generated_caption}")
        print(f"{'='*70}")

    print("\n‚úÖ SCRIPT TERMIN√â AVEC SUCC√àS ! üéâ\n")


if __name__ == "__main__":
    main()
